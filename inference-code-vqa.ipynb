{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8423775,"sourceType":"datasetVersion","datasetId":4950186},{"sourceId":8444943,"sourceType":"datasetVersion","datasetId":5031972},{"sourceId":8446856,"sourceType":"datasetVersion","datasetId":5033268},{"sourceId":8447364,"sourceType":"datasetVersion","datasetId":5033625}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom keras.models import Model\nfrom keras.applications.vgg19 import VGG19, preprocess_input\nfrom keras.utils import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer, AutoFeatureExtractor, AutoModel,            \n    TrainingArguments, Trainer, logging\n)\nfrom datasets import load_dataset, set_caching_enabled, Dataset\n# from nltk.corpus import wordnet\n\n# # nltk setup\n# nltk.download('wordnet')\n\n# Environment setup\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n# set_caching_enabled(True)\n# Logging setup\nlogging.set_verbosity_error()\n\n# Device setup\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sampled-dataset-for-vqa/sampled_train_dataset_kaggle.csv\")\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/sampled-dataset-for-vqa/sampled_validation_dataset_kaggle.csv\")\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_vocabulary = list(df['multiple_choice_answer'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_encoder(word):\n    if word in list_vocabulary:\n        index = list_vocabulary.index(word)\n        return index\n    else:\n        return len(list_vocabulary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['labels'] = df['multiple_choice_answer'].apply(lambda x: label_encoder(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['labels'] = test_df['multiple_choice_answer'].apply(lambda x: label_encoder(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df\ndf_test = test_df.iloc[1500:1503]\ndf_train.to_csv(\"data_train.csv\", index=None)\ndf_test.to_csv(\"data_eval.csv\", index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\n    \"csv\", \n    data_files={\n        \"train\": \"data_train.csv\",\n        \"test\": \"data_eval.csv\"\n    }\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install peft -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    init_lora_weights=\"gaussian\",\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"out_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],\n)\nconfig2 = LoraConfig(\n    init_lora_weights=\"gaussian\",\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"query\", \"value\", \"linear\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_list = []\nimage_list = []\npredictions_list = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForwardNeuralNetwork(nn.Module):\n    def __init__(self,embedding_dim):\n        super(FeedForwardNeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim,2*embedding_dim)\n        self.fc2 = nn.Linear(2*embedding_dim,embedding_dim)\n        self.activation_function = nn.GELU()\n#         self.layer_norm = nn.LayerNorm(embedding_dim)\n    def forward(self,input_data):\n        output = self.fc1(input_data)\n        output = self.activation_function(output)\n        output = self.fc2(output)\n        output = self.activation_function(output)\n#         output = self.layer_norm(output)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionBert(nn.Module):\n    def __init__(self,hidden_size, num_heads):\n        super(AttentionBert, self).__init__()\n#         self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.layer_norm1 = nn.LayerNorm(self.hidden_size).to(device)\n        self.ffn = FeedForwardNeuralNetwork(self.hidden_size).to(device)\n#         self.query_linear = nn.Linear(input_size, hidden_size)\n#         self.key_linear = nn.Linear(input_size, hidden_size)\n#         self.value_linear = nn.Linear(input_size, hidden_size)\n        \n        self.multihead_attention = nn.MultiheadAttention(self.hidden_size, self.num_heads).to(device)\n        print(self.multihead_attention)\n    def forward(self, query,input_features):\n        # Transform query, key, and value\n#         query = self.query_linear(query)\n#         key = self.key_linear(input_features)\n#         value = self.value_linear(input_features)\n        query = query\n        key = input_features\n        value = input_features\n        \n        # Transpose for multihead attention\n        query = query.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n        key = key.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n        value = value.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n#         print(query.get_device())\n#         print(key.get_device())\n#         print(value.get_device())\n        # Compute co-attention and add to the query of the layer\n        co_attention_output, _ = self.multihead_attention(query, key, value)\n#         query = query.transpose(0, 1)\n        residual_output = torch.add(co_attention_output,query)\n        \n        # Transpose back to original shape and normalize\n        residual_output = residual_output.transpose(0, 1) # (batch_size, seq_len, hidden_size)\n#         print(residual_output.get_device())\n        normalized_residual_output = self.layer_norm1(residual_output)\n        \n        #Send input to feedforward neural network and add and nor\n        feedforwardoutput = self.ffn.forward(normalized_residual_output)\n        residual_output2 = torch.add(feedforwardoutput,normalized_residual_output)\n#         print(residual_output2.get_device())\n        normalized_residual_output2 = self.layer_norm1(residual_output2)\n        \n        return normalized_residual_output2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SelfTRM(nn.Module):\n    def __init__(self,num_layers,num_heads,input_size):\n        super(SelfTRM,self).__init__()\n        self.num_layers = num_layers\n        self.layers = []\n        for i in range(self.num_layers):\n#             self.layers.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n#             AttentionBert(input_size,num_heads)\n# get_peft_model(AttentionBert(input_size,num_heads),config)\n            self.layers.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n#         print(self.layers[0])\n#             self.layers.append(LoRAAttentionBert(input_size,num_heads))\n    def forward(self,input):\n#         print(input.get_device())\n        for i in range(self.num_layers):\n            output = self.layers[i].forward(input,input)\n            input = output\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP_classifier(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(MLP_classifier,self).__init__()\n        self.linear1 = nn.Linear(input_size,input_size*2)\n        self.activation_function1 =  nn.GELU()\n        self.linear2 = nn.Linear(input_size*2,output_size)\n        self.activation_function2 = nn.Softmax()\n    def forward(self,input):\n        output = self.linear1(input)\n        output = self.activation_function1(output)\n        output = self.linear2(output)\n        return output\n#         output = self.activation_function2(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiattentionforImageandWord(nn.Module):\n    def __init__(self,num_layers,num_heads,input_size):\n        super(BiattentionforImageandWord,self).__init__()\n        self.layers_image = []\n        self.layers_word = []\n        self.input_size = input_size\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n#         get_peft_model(AttentionBert(input_size,num_heads),config)\n        for i in range(self.num_layers):\n#             \n#             self.layers_image.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n            self.layers_image.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n#             self.layers_image.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n            self.layers_image.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n#             self.layers_word.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n            self.layers_word.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n#             self.layers_word.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n            self.layers_word.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n    def forward(self,query_image,key_image,query_word,key_word):\n        for i in range(self.num_layers):\n            output_image1 = self.layers_image[2*i].forward(query_image,key_word)\n            output_image2 = self.layers_image[2*i+1].forward(output_image1,output_image1)\n            output_word1 = self.layers_word[2*i].forward(query_word,key_image)\n            output_word2 = self.layers_word[2*i+1].forward(output_word1,output_word1)\n            query_image = key_image = output_image2\n            query_word = key_word = output_word2\n        return output_image2,output_word2    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultimodalBert(nn.Module):\n    def __init__(self,text,image,num_layers,num_heads,input_size,output_size):\n        super(MultimodalBert,self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.word_model = AutoModel.from_pretrained(text)\n        self.image_model = AutoModel.from_pretrained(image)\n        self.CoTRM = BiattentionforImageandWord(num_layers,num_heads,self.input_size)\n        self.wordTRM = SelfTRM(num_layers,num_heads,self.input_size)\n        self.MLPclassifier = MLP_classifier(self.input_size*2,self.output_size)\n        self.criterion = nn.CrossEntropyLoss()\n    def forward(self,\n            input_ids: torch.LongTensor,\n            pixel_values: torch.FloatTensor,\n            attention_mask: Optional[torch.LongTensor] = None,\n            token_type_ids: Optional[torch.LongTensor] = None,\n            labels: Optional[torch.LongTensor] = None):\n        global predictions_list\n        output = self.word_model(input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True,)\n#                 output =  self.word_model(**encoded_text)\n        word_features = output.last_hidden_state\n        output = self.image_model(\n            pixel_values=pixel_values,\n            return_dict=True,\n        )\n        image_features = output.last_hidden_state\n        #     (self,image_features,word_features,labels = None):\n        \n        #Self Transformer before CoTRM\n        self_trm_output = self.wordTRM.forward(word_features)\n        #CoTRM Portion\n        CoTRM_word_input = self_trm_output \n        CoTRM_image_input = image_features\n        CoTRM_output_image,CoTRM_output_word = self.CoTRM.forward(CoTRM_image_input,CoTRM_image_input,CoTRM_word_input,CoTRM_word_input)\n        # Merged output and MLP classifier\n        merged_input = torch.cat([CoTRM_output_image[:,0,:],CoTRM_output_word[:,0,:]],dim=1)\n        final_output = self.MLPclassifier.forward(merged_input)\n#         print(final_output.predictions)\n#         class_outputs = np.argmax(final_output,axis = 1)\n        predictions_list = final_output\n        out = {\n            \"out\": final_output\n        }\n        if labels is not None:\n            loss = self.criterion(final_output, labels)\n            out[\"loss\"] = loss\n#             out[\"labels\"] = labels\n#         print(\"Printing Labels\")\n#         print(labels)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The dataclass decorator is used to automatically generate special methods to classes, \n# including __init__, __str__ and __repr__. It helps reduce some boilerplate code.\n@dataclass\nclass MultimodalCollator:\n    tokenizer: AutoTokenizer\n\n    preprocessor: AutoFeatureExtractor\n        \n    def tokenize_text(self, texts: List[str]):\n        #Encoded text \n#         global question_list\n#         question_list = texts\n        encoded_text = self.tokenizer(\n            text=texts, \n            padding='longest',\n            max_length=40,\n            truncation=True,\n            return_tensors='pt',\n            return_token_type_ids=True,\n            return_attention_mask=True,\n        )\n#         output =  self.word_model(**encoded_text)\n#         word_features = output.last_hidden_state\n#         return {\n#             \"word_features\": word_features\n#         }\n        return {\n                \"input_ids\": encoded_text['input_ids'].squeeze(),\n#                 \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n#                 \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n            }\n\n    def preprocess_images(self, images: List[str]):\n        #Fixed\n#         print(\"Done\")\n        global image_list\n        image_list = [Image.open(os.path.join(\"/kaggle/input/sampled-dataset-for-vqa\", image_name.replace(\"\\\\\", \"/\"))).convert('RGB') for image_name in images]\n        processed_images = self.preprocessor(\n            images=[Image.open(os.path.join(\"/kaggle/input/sampled-dataset-for-vqa\", image_name.replace(\"\\\\\", \"/\"))).convert('RGB') for image_name in images],\n            return_tensors=\"pt\",\n        )\n#         print(\"done2\")\n#         output = self.image_model(**processed_images) #Of the form (Num_images,Num_tokens,Hidden_size)\n#         image_features = output.last_hidden_state\n#         return{\n#             \"image_features\": image_features\n#         }\n        return {\n            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n        }\n            \n    def __call__(self, raw_batch_dict):\n        #Fix the label return value\n        return {\n            **self.tokenize_text(\n                raw_batch_dict['question']\n                if isinstance(raw_batch_dict, dict) else\n                [i['question'] for i in raw_batch_dict]\n            ),\n            **self.preprocess_images(\n                raw_batch_dict[\"kaggle_image_path\"]\n                if isinstance(raw_batch_dict, dict) else\n                [i[\"kaggle_image_path\"] for i in raw_batch_dict]\n            ),\n            \"labels\": torch.tensor(\n                raw_batch_dict['labels']\n                if isinstance(raw_batch_dict, dict) else\n                [i['labels'] for i in raw_batch_dict],\n                dtype=torch.int64\n            ),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k',num_layers = 3,num_heads=3,hidden_size = 768,vocabulary_size =9129):\n    # Initialize the correct text tokenizer and image feature extractor, and use them to create the collator\n    tokenizer = AutoTokenizer.from_pretrained(text)\n#     word_model = AutoModel.from_pretrained(text)\n    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n#     image_model = AutoModel.from_pretrained(image)\n    multimodal_collator = MultimodalCollator(tokenizer=tokenizer, preprocessor=preprocessor)\n    \n    # Initialize the multimodal model with the appropriate weights from pretrained models\n    multimodal_model = MultimodalBert(text,image,num_layers = num_layers,num_heads=num_heads,input_size = hidden_size,output_size = vocabulary_size).to(device)\n#     multimodal_model = multimodal_model.to(device)\n    return multimodal_collator, multimodal_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate\n\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load('recall')\naccuracy_metric = evaluate.load(\"accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wrapper around the wup_measure(...) function to process batch inputs\n# def batch_wup_measure(labels, preds):\n#     wup_scores = [wup_measure(label_encoder(label), label_encoder(pred)) for label, pred in zip(labels, preds)]\n#     return np.mean(wup_scores)\n\n# # Function to compute all relevant performance metrics, to be passed into the trainer\n# def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n#     logits, labels = eval_tuple\n#     preds = logits.argmax(axis=-1)\n#     return {\n# #         \"wups\": batch_wup_measure(labels, preds),\n#         \"acc\": accuracy_score(labels, preds),\n#         \"f1\": f1_score(labels, preds, average='weighted')\n#     }\nfrom sklearn.metrics import recall_score\ndef compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]):\n    \"\"\"\n    Computes evaluation metrics for a given set of logits and labels.\n\n    Args:\n        eval_tuple (Tuple): Tuple containing logits and corresponding ground truth labels.\n\n    Returns:\n        Dict: Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n    \"\"\"\n    logits, labels = eval_tuple\n\n    # Calculate predictions\n    preds = logits.argmax(axis=-1)\n#     print(preds)\n#     print(\"Done\")\n#     print(\"\\n\")\n#     Compute metrics\n    metrics = {\n        \"eval_acc\": accuracy_metric.compute(predictions=preds, references=labels),\n        \"eval_f1\": f1_score(labels, preds, average='weighted'),\n        \"eval_precision\": precision_metric.compute(predictions=preds, references=labels,average='weighted'),\n        \"eval_recall\": recall_score(labels, preds, average='weighted'),\n    }\n    return metrics\n# #     print( metric.compute(predictions=preds, references=labels))\n#     return metric.compute(predictions=preds, references=labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_args = TrainingArguments(\n    output_dir=\"checkpoint\",\n    seed=12345, \n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_steps=1000,\n    save_total_limit=1,             # Since models are large, save only the last 3 checkpoints at any given time while training \n    metric_for_best_model=\"accuracy\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    remove_unused_columns=False,\n    num_train_epochs=5,\n    fp16=False,\n    dataloader_num_workers=4,\n    load_best_model_at_end=True,\n    report_to=\"tensorboard\",\n    label_names=[\"labels\"]\n)\n\n# Initialize the actual collator and multimodal model\ncollator, model = createMultimodalVQACollatorAndModel(\"bert-base-uncased\", \"google/vit-base-patch16-224-in21k\")\n# model = replace_lora_weights_loftq(get_peft_model(model,config2))\nlora_model = get_peft_model(model,config2)\n# lora_model = model\n# print(lora_model)\ncheckpoint = \"/kaggle/input/vqa-models-finalproject/pytorch_model.bin\"\nlora_model.load_state_dict(torch.load(checkpoint))\nlora_model.to(device)\n\n# def print_trainable_parameters(model):\n#     trainable_params = 0\n#     all_param = 0\n#     for _, param in model.named_parameters():\n#         all_param += param.numel()\n#         if param.requires_grad:\n#             trainable_params += param.numel()\n#     print(\n#         f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n#     )\n    \n# print_trainable_parameters(model)\n\n# Initialize the trainer with the dataset, collator, model, hyperparameters and evaluation metrics\nmulti_trainer = Trainer(\n\tlora_model,\n\tmulti_args,\n\ttrain_dataset=dataset['train'],\n\teval_dataset=dataset['test'],\n\tdata_collator=collator,\n\tcompute_metrics=compute_metrics,\n)\nprint(multi_trainer.compute_metrics)\n# for batch in multi_trainer.get_eval_dataloader():\n#     print(batch)\n#     break\n# Start the training loop\n# train_multi_metrics = multi_trainer.train()\n\n# Run the model on the evaluation set to obtain final metrics\neval_multi_metrics = multi_trainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# image_list = []\n# predictions_list = []\nprint(predictions_list)\npredictions_list = predictions_list.cpu()\nimages=[Image.open(os.path.join(\"/kaggle/input/sampled-dataset-for-vqa\", image_name.replace(\"\\\\\", \"/\"))).convert('RGB') for image_name in dataset[\"test\"][\"kaggle_image_path\"]]\n# print(type(images[0]))\nprint(len(images))\n\nquestion_list = dataset[\"test\"]['question']\nfor i in range(len(predictions_list)):\n#     images[i].show()\n    plt.imshow(images[i])\n    print(question_list[i])\n    if np.argmax(predictions_list[0][i])<len(list_vocabulary):\n        print(list_vocabulary[np.argmax(predictions_list[i])])\n    else:\n        print(\"Unknown\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.remove(\"/kaggle/working/data_train.csv\")\nos.remove(\"/kaggle/working/data_eval.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df\ndf_test = test_df\ndf_train.to_csv(\"data_train.csv\", index=None)\ndf_test.to_csv(\"data_eval.csv\", index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\n    \"csv\", \n    data_files={\n        \"train\": \"data_train.csv\",\n        \"test\": \"data_eval.csv\"\n    }\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the trainer with the dataset, collator, model, hyperparameters and evaluation metrics\nmulti_trainer = Trainer(\n\tlora_model,\n\tmulti_args,\n\ttrain_dataset=dataset['train'],\n\teval_dataset=dataset['test'],\n\tdata_collator=collator,\n\tcompute_metrics=compute_metrics,\n)\nprint(multi_trainer.compute_metrics)\n# for batch in multi_trainer.get_eval_dataloader():\n#     print(batch)\n#     break\n# Start the training loop\n# train_multi_metrics = multi_trainer.train()\n\n# Run the model on the evaluation set to obtain final metrics\neval_multi_metrics = multi_trainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}