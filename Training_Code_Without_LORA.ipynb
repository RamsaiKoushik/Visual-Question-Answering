{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:06.878426Z","iopub.status.busy":"2024-05-17T12:49:06.878031Z","iopub.status.idle":"2024-05-17T12:49:30.447792Z","shell.execute_reply":"2024-05-17T12:49:30.446719Z","shell.execute_reply.started":"2024-05-17T12:49:06.878396Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","import pickle\n","import nltk\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from keras.callbacks import TensorBoard\n","\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from keras.models import Model\n","from keras.applications.vgg19 import VGG19, preprocess_input\n","from keras.utils import load_img, img_to_array\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from copy import deepcopy\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoTokenizer, AutoFeatureExtractor, AutoModel,            \n","    TrainingArguments, Trainer, logging\n",")\n","from datasets import load_dataset, set_caching_enabled, Dataset\n","# from nltk.corpus import wordnet\n","\n","# # nltk setup\n","# nltk.download('wordnet')\n","\n","# Environment setup\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","# os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","# set_caching_enabled(True)\n","# Logging setup\n","logging.set_verbosity_error()\n","\n","# Device setup\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:30.458264Z","iopub.status.busy":"2024-05-17T12:49:30.457856Z","iopub.status.idle":"2024-05-17T12:49:32.401219Z","shell.execute_reply":"2024-05-17T12:49:32.400038Z","shell.execute_reply.started":"2024-05-17T12:49:30.458226Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/sampled-dataset-for-vqa/sampled_train_dataset_kaggle.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:32.404362Z","iopub.status.busy":"2024-05-17T12:49:32.403993Z","iopub.status.idle":"2024-05-17T12:49:32.422052Z","shell.execute_reply":"2024-05-17T12:49:32.420988Z","shell.execute_reply.started":"2024-05-17T12:49:32.404330Z"},"trusted":true},"outputs":[],"source":["df[df['hashed_image_id']==5]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:32.429820Z","iopub.status.busy":"2024-05-17T12:49:32.429363Z","iopub.status.idle":"2024-05-17T12:49:32.828742Z","shell.execute_reply":"2024-05-17T12:49:32.827692Z","shell.execute_reply.started":"2024-05-17T12:49:32.429781Z"},"trusted":true},"outputs":[],"source":["dir_path = \"/kaggle/input/sampled-dataset-for-vqa\"\n","questions = df['question']\n","\n","images = []\n","\n","for image_path in tqdm(df[\"kaggle_image_path\"]):\n","#     print(image_path)\n","    images.append(os.path.join(dir_path, image_path))\n","# print(image_path)\n","# questions\n","# images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:32.830648Z","iopub.status.busy":"2024-05-17T12:49:32.830245Z","iopub.status.idle":"2024-05-17T12:49:33.780276Z","shell.execute_reply":"2024-05-17T12:49:33.779154Z","shell.execute_reply.started":"2024-05-17T12:49:32.830608Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(\"/kaggle/input/sampled-dataset-for-vqa/sampled_validation_dataset_kaggle.csv\")\n","test_df"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the classification problem on the visual question answering portion"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:33.807600Z","iopub.status.busy":"2024-05-17T12:49:33.807232Z","iopub.status.idle":"2024-05-17T12:49:33.828609Z","shell.execute_reply":"2024-05-17T12:49:33.827453Z","shell.execute_reply.started":"2024-05-17T12:49:33.807570Z"},"trusted":true},"outputs":[],"source":["list_vocabulary = list(df['multiple_choice_answer'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:33.830969Z","iopub.status.busy":"2024-05-17T12:49:33.830290Z","iopub.status.idle":"2024-05-17T12:49:33.837925Z","shell.execute_reply":"2024-05-17T12:49:33.836750Z","shell.execute_reply.started":"2024-05-17T12:49:33.830925Z"},"trusted":true},"outputs":[],"source":["def label_encoder(word):\n","    if word in list_vocabulary:\n","        index = list_vocabulary.index(word)\n","        return index\n","    else:\n","        return len(list_vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:33.839392Z","iopub.status.busy":"2024-05-17T12:49:33.839055Z","iopub.status.idle":"2024-05-17T12:49:33.850039Z","shell.execute_reply":"2024-05-17T12:49:33.848736Z","shell.execute_reply.started":"2024-05-17T12:49:33.839359Z"},"trusted":true},"outputs":[],"source":["print(len(list_vocabulary))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:33.851809Z","iopub.status.busy":"2024-05-17T12:49:33.851444Z","iopub.status.idle":"2024-05-17T12:49:33.878559Z","shell.execute_reply":"2024-05-17T12:49:33.877252Z","shell.execute_reply.started":"2024-05-17T12:49:33.851779Z"},"trusted":true},"outputs":[],"source":["# df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:33.880521Z","iopub.status.busy":"2024-05-17T12:49:33.880150Z","iopub.status.idle":"2024-05-17T12:49:36.589370Z","shell.execute_reply":"2024-05-17T12:49:36.588236Z","shell.execute_reply.started":"2024-05-17T12:49:33.880489Z"},"trusted":true},"outputs":[],"source":["df['Labels'] = df['multiple_choice_answer'].apply(lambda x: label_encoder(x))\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:36.590909Z","iopub.status.busy":"2024-05-17T12:49:36.590568Z","iopub.status.idle":"2024-05-17T12:49:37.893324Z","shell.execute_reply":"2024-05-17T12:49:37.892168Z","shell.execute_reply.started":"2024-05-17T12:49:36.590877Z"},"trusted":true},"outputs":[],"source":["test_df['Labels'] = test_df['multiple_choice_answer'].apply(lambda x: label_encoder(x))\n","test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:37.895893Z","iopub.status.busy":"2024-05-17T12:49:37.895109Z","iopub.status.idle":"2024-05-17T12:49:37.900780Z","shell.execute_reply":"2024-05-17T12:49:37.899463Z","shell.execute_reply.started":"2024-05-17T12:49:37.895850Z"},"trusted":true},"outputs":[],"source":["# df[df['image_id']==25]\n","# /kaggle/input/sampled-dataset-for-vqa/train2014/train2014/COCO_train2014_000000000025.jpg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:37.902483Z","iopub.status.busy":"2024-05-17T12:49:37.902161Z","iopub.status.idle":"2024-05-17T12:49:37.930619Z","shell.execute_reply":"2024-05-17T12:49:37.929422Z","shell.execute_reply.started":"2024-05-17T12:49:37.902453Z"},"trusted":true},"outputs":[],"source":["df_train = df\n","df_test = test_df\n","df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:37.932646Z","iopub.status.busy":"2024-05-17T12:49:37.932249Z","iopub.status.idle":"2024-05-17T12:49:45.045367Z","shell.execute_reply":"2024-05-17T12:49:45.044448Z","shell.execute_reply.started":"2024-05-17T12:49:37.932613Z"},"trusted":true},"outputs":[],"source":["\n","df_train.to_csv(\"data_train.csv\", index=None)\n","df_test.to_csv(\"data_eval.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:45.046912Z","iopub.status.busy":"2024-05-17T12:49:45.046596Z","iopub.status.idle":"2024-05-17T12:49:47.453921Z","shell.execute_reply":"2024-05-17T12:49:47.452966Z","shell.execute_reply.started":"2024-05-17T12:49:45.046885Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\n","    \"csv\", \n","    data_files={\n","        \"train\": \"data_train.csv\",\n","        \"test\": \"data_eval.csv\"\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.455392Z","iopub.status.busy":"2024-05-17T12:49:47.455064Z","iopub.status.idle":"2024-05-17T12:49:47.460945Z","shell.execute_reply":"2024-05-17T12:49:47.460044Z","shell.execute_reply.started":"2024-05-17T12:49:47.455363Z"},"trusted":true},"outputs":[],"source":["print(dataset['train'])"]},{"cell_type":"markdown","metadata":{},"source":["# Multimodal Coattention Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.462744Z","iopub.status.busy":"2024-05-17T12:49:47.462389Z","iopub.status.idle":"2024-05-17T12:49:47.477064Z","shell.execute_reply":"2024-05-17T12:49:47.475917Z","shell.execute_reply.started":"2024-05-17T12:49:47.462693Z"},"trusted":true},"outputs":[],"source":["class FeedForwardNeuralNetwork(nn.Module):\n","    def __init__(self,embedding_dim):\n","        super(FeedForwardNeuralNetwork, self).__init__()\n","        self.fc1 = nn.Linear(embedding_dim,2*embedding_dim)\n","        self.fc2 = nn.Linear(2*embedding_dim,embedding_dim)\n","        self.activation_function = nn.GELU()\n","#         self.layer_norm = nn.LayerNorm(embedding_dim)\n","    def forward(self,input_data):\n","        output = self.fc1(input_data)\n","        output = self.activation_function(output)\n","        output = self.fc2(output)\n","        output = self.activation_function(output)\n","#         output = self.layer_norm(output)\n","        return output\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.479108Z","iopub.status.busy":"2024-05-17T12:49:47.478649Z","iopub.status.idle":"2024-05-17T12:49:47.497658Z","shell.execute_reply":"2024-05-17T12:49:47.496394Z","shell.execute_reply.started":"2024-05-17T12:49:47.479066Z"},"trusted":true},"outputs":[],"source":["class AttentionBert(nn.Module):\n","    def __init__(self,hidden_size, num_heads):\n","        super(AttentionBert, self).__init__()\n","#         self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.layer_norm1 = nn.LayerNorm(self.hidden_size).to(device)\n","        self.ffn = FeedForwardNeuralNetwork(self.hidden_size).to(device)\n","#         self.query_linear = nn.Linear(input_size, hidden_size)\n","#         self.key_linear = nn.Linear(input_size, hidden_size)\n","#         self.value_linear = nn.Linear(input_size, hidden_size)\n","        \n","        self.multihead_attention = nn.MultiheadAttention(self.hidden_size, self.num_heads).to(device)\n","    def forward(self, query,input_features):\n","        # Transform query, key, and value\n","#         query = self.query_linear(query)\n","#         key = self.key_linear(input_features)\n","#         value = self.value_linear(input_features)\n","        query = query\n","        key = input_features\n","        value = input_features\n","        \n","        # Transpose for multihead attention\n","        query = query.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        key = key.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        value = value.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","#         print(query.get_device())\n","#         print(key.get_device())\n","#         print(value.get_device())\n","        # Compute co-attention and add to the query of the layer\n","        co_attention_output, _ = self.multihead_attention(query, key, value)\n","#         query = query.transpose(0, 1)\n","        residual_output = torch.add(co_attention_output,query)\n","        \n","        # Transpose back to original shape and normalize\n","        residual_output = residual_output.transpose(0, 1) # (batch_size, seq_len, hidden_size)\n","#         print(residual_output.get_device())\n","        normalized_residual_output = self.layer_norm1(residual_output)\n","        \n","        #Send input to feedforward neural network and add and nor\n","        feedforwardoutput = self.ffn.forward(normalized_residual_output)\n","        residual_output2 = torch.add(feedforwardoutput,normalized_residual_output)\n","#         print(residual_output2.get_device())\n","        normalized_residual_output2 = self.layer_norm1(residual_output2)\n","        \n","        return normalized_residual_output2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.499267Z","iopub.status.busy":"2024-05-17T12:49:47.498910Z","iopub.status.idle":"2024-05-17T12:49:47.515911Z","shell.execute_reply":"2024-05-17T12:49:47.514741Z","shell.execute_reply.started":"2024-05-17T12:49:47.499238Z"},"trusted":true},"outputs":[],"source":["class SelfTRM(nn.Module):\n","    def __init__(self,num_layers,num_heads,input_size):\n","        super(SelfTRM,self).__init__()\n","        self.num_layers = num_layers\n","        self.layers = []\n","        for i in range(self.num_layers):\n","            self.layers.append(AttentionBert(input_size,num_heads))\n","    def forward(self,input):\n","#         print(input.get_device())\n","        for i in range(self.num_layers):\n","            output = self.layers[i].forward(input,input)\n","            input = output\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.518121Z","iopub.status.busy":"2024-05-17T12:49:47.517732Z","iopub.status.idle":"2024-05-17T12:49:47.530857Z","shell.execute_reply":"2024-05-17T12:49:47.529634Z","shell.execute_reply.started":"2024-05-17T12:49:47.518088Z"},"trusted":true},"outputs":[],"source":["class MLP_classifier(nn.Module):\n","    def __init__(self,input_size,output_size):\n","        super(MLP_classifier,self).__init__()\n","        self.linear1 = nn.Linear(input_size,input_size*2)\n","        self.activation_function1 =  nn.GELU()\n","        self.linear2 = nn.Linear(input_size*2,output_size)\n","        self.activation_function2 = nn.Softmax()\n","    def forward(self,input):\n","        output = self.linear1(input)\n","        output = self.activation_function1(output)\n","        output = self.linear2(output)\n","        return output\n","#         output = self.activation_function2(output)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.532793Z","iopub.status.busy":"2024-05-17T12:49:47.532291Z","iopub.status.idle":"2024-05-17T12:49:47.545062Z","shell.execute_reply":"2024-05-17T12:49:47.543975Z","shell.execute_reply.started":"2024-05-17T12:49:47.532755Z"},"trusted":true},"outputs":[],"source":["class BiattentionforImageandWord(nn.Module):\n","    def __init__(self,num_layers,num_heads,input_size):\n","        super(BiattentionforImageandWord,self).__init__()\n","        self.layers_image = []\n","        self.layers_word = []\n","        self.input_size = input_size\n","        self.num_heads = num_heads\n","        self.num_layers = num_layers\n","        for i in range(self.num_layers):\n","            self.layers_image.append(AttentionBert(input_size,num_heads))\n","            self.layers_image.append(AttentionBert(input_size,num_heads))\n","            self.layers_word.append(AttentionBert(input_size,num_heads))\n","            self.layers_word.append(AttentionBert(input_size,num_heads))\n","    def forward(self,query_image,key_image,query_word,key_word):\n","        for i in range(self.num_layers):\n","            output_image1 = self.layers_image[2*i].forward(query_image,key_word)\n","            output_image2 = self.layers_image[2*i+1].forward(output_image1,output_image1)\n","            output_word1 = self.layers_word[2*i].forward(query_word,key_image)\n","            output_word2 = self.layers_word[2*i+1].forward(output_word1,output_word1)\n","            query_image = key_image = output_image2\n","            query_word = key_word = output_word2\n","        return output_image2,output_word2    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.548871Z","iopub.status.busy":"2024-05-17T12:49:47.548476Z","iopub.status.idle":"2024-05-17T12:49:47.565608Z","shell.execute_reply":"2024-05-17T12:49:47.564193Z","shell.execute_reply.started":"2024-05-17T12:49:47.548812Z"},"trusted":true},"outputs":[],"source":["class MultimodalBert(nn.Module):\n","    def __init__(self,text,image,num_layers,num_heads,input_size,output_size):\n","        super(MultimodalBert,self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.word_model = AutoModel.from_pretrained(text)\n","        self.image_model = AutoModel.from_pretrained(image)\n","        self.CoTRM = BiattentionforImageandWord(num_layers,num_heads,self.input_size)\n","        self.wordTRM = SelfTRM(num_layers,num_heads,self.input_size)\n","        self.MLPclassifier = MLP_classifier(self.input_size*2,self.output_size)\n","        self.criterion = nn.CrossEntropyLoss()\n","    def forward(self,\n","            input_ids: torch.LongTensor,\n","            pixel_values: torch.FloatTensor,\n","            attention_mask: Optional[torch.LongTensor] = None,\n","            token_type_ids: Optional[torch.LongTensor] = None,\n","            labels: Optional[torch.LongTensor] = None):\n","        output = self.word_model(input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=True,)\n","#                 output =  self.word_model(**encoded_text)\n","        word_features = output.last_hidden_state\n","        output = self.image_model(\n","            pixel_values=pixel_values,\n","            return_dict=True,\n","        )\n","        image_features = output.last_hidden_state\n","        #     (self,image_features,word_features,labels = None):\n","        \n","        #Self Transformer before CoTRM\n","        self_trm_output = self.wordTRM.forward(word_features)\n","        #CoTRM Portion\n","        CoTRM_word_input = self_trm_output \n","        CoTRM_image_input = image_features\n","        CoTRM_output_image,CoTRM_output_word = self.CoTRM.forward(CoTRM_image_input,CoTRM_image_input,CoTRM_word_input,CoTRM_word_input)\n","        # Merged output and MLP classifier\n","        merged_input = torch.cat([CoTRM_output_image[:,0,:],CoTRM_output_word[:,0,:]],dim=1)\n","        final_output = self.MLPclassifier.forward(merged_input)\n","#         class_outputs = np.argmax(final_output,axis = 1)\n","        out = {\n","            \"out\": final_output\n","        }\n","        if labels is not None:\n","            loss = self.criterion(final_output, labels)\n","            out[\"loss\"] = loss\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.567916Z","iopub.status.busy":"2024-05-17T12:49:47.567467Z","iopub.status.idle":"2024-05-17T12:49:47.585259Z","shell.execute_reply":"2024-05-17T12:49:47.583904Z","shell.execute_reply.started":"2024-05-17T12:49:47.567877Z"},"trusted":true},"outputs":[],"source":["# The dataclass decorator is used to automatically generate special methods to classes, \n","# including __init__, __str__ and __repr__. It helps reduce some boilerplate code.\n","@dataclass\n","class MultimodalCollator:\n","    tokenizer: AutoTokenizer\n","\n","    preprocessor: AutoFeatureExtractor\n","        \n","    def tokenize_text(self, texts: List[str]):\n","        #Encoded text \n","        encoded_text = self.tokenizer(\n","            text=texts, \n","            padding='longest',\n","            max_length=40,\n","            truncation=True,\n","            return_tensors='pt',\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","        )\n","#         output =  self.word_model(**encoded_text)\n","#         word_features = output.last_hidden_state\n","#         return {\n","#             \"word_features\": word_features\n","#         }\n","        return {\n","                \"input_ids\": encoded_text['input_ids'].squeeze(),\n","#                 \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n","#                 \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n","            }\n","\n","    def preprocess_images(self, images: List[str]):\n","        #Fixed\n","#         print(\"Done\")\n","        \n","        processed_images = self.preprocessor(\n","            images=[Image.open(os.path.join(\"/kaggle/input/sampled-dataset-for-vqa\", image_name.replace(\"\\\\\", \"/\"))).convert('RGB') for image_name in images],\n","            return_tensors=\"pt\",\n","        )\n","#         print(\"done2\")\n","#         output = self.image_model(**processed_images) #Of the form (Num_images,Num_tokens,Hidden_size)\n","#         image_features = output.last_hidden_state\n","#         return{\n","#             \"image_features\": image_features\n","#         }\n","        return {\n","            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n","        }\n","            \n","    def __call__(self, raw_batch_dict):\n","        #Fix the label return value\n","        return {\n","            **self.tokenize_text(\n","                raw_batch_dict['question']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['question'] for i in raw_batch_dict]\n","            ),\n","            **self.preprocess_images(\n","                raw_batch_dict[\"kaggle_image_path\"]\n","                if isinstance(raw_batch_dict, dict) else\n","                [i[\"kaggle_image_path\"] for i in raw_batch_dict]\n","            ),\n","            'labels': torch.tensor(\n","                raw_batch_dict['Labels']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['Labels'] for i in raw_batch_dict],\n","                dtype=torch.int64\n","            ),\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.594024Z","iopub.status.busy":"2024-05-17T12:49:47.593609Z","iopub.status.idle":"2024-05-17T12:49:47.602243Z","shell.execute_reply":"2024-05-17T12:49:47.601099Z","shell.execute_reply.started":"2024-05-17T12:49:47.593989Z"},"trusted":true},"outputs":[],"source":["def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k',num_layers = 3,num_heads=3,hidden_size = 768,vocabulary_size =9129):\n","    # Initialize the correct text tokenizer and image feature extractor, and use them to create the collator\n","    tokenizer = AutoTokenizer.from_pretrained(text)\n","#     word_model = AutoModel.from_pretrained(text)\n","    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n","#     image_model = AutoModel.from_pretrained(image)\n","    multimodal_collator = MultimodalCollator(tokenizer=tokenizer, preprocessor=preprocessor)\n","    \n","    # Initialize the multimodal model with the appropriate weights from pretrained models\n","    multimodal_model = MultimodalBert(text,image,num_layers = num_layers,num_heads=num_heads,input_size = hidden_size,output_size = vocabulary_size).to(device)\n","#     multimodal_model = multimodal_model.to(device)\n","    return multimodal_collator, multimodal_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:49:47.603759Z","iopub.status.busy":"2024-05-17T12:49:47.603366Z","iopub.status.idle":"2024-05-17T12:50:04.980191Z","shell.execute_reply":"2024-05-17T12:50:04.979056Z","shell.execute_reply.started":"2024-05-17T12:49:47.603714Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate\n","\n","import evaluate\n","\n","metric = evaluate.load(\"accuracy\")\n","precision_metric = evaluate.load(\"precision\")\n","recall_metric = evaluate.load('recall')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:50:04.982014Z","iopub.status.busy":"2024-05-17T12:50:04.981644Z","iopub.status.idle":"2024-05-17T12:50:04.990351Z","shell.execute_reply":"2024-05-17T12:50:04.989306Z","shell.execute_reply.started":"2024-05-17T12:50:04.981982Z"},"trusted":true},"outputs":[],"source":["# # Wrapper around the wup_measure(...) function to process batch inputs\n","# # def batch_wup_measure(labels, preds):\n","# #     wup_scores = [wup_measure(label_encoder(label), label_encoder(pred)) for label, pred in zip(labels, preds)]\n","# #     return np.mean(wup_scores)\n","\n","# # # Function to compute all relevant performance metrics, to be passed into the trainer\n","# # def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n","# #     logits, labels = eval_tuple\n","# #     preds = logits.argmax(axis=-1)\n","# #     return {\n","# # #         \"wups\": batch_wup_measure(labels, preds),\n","# #         \"acc\": accuracy_score(labels, preds),\n","# #         \"f1\": f1_score(labels, preds, average='weighted')\n","# #     }\n","# def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]):\n","#     \"\"\"\n","#     Computes evaluation metrics for a given set of logits and labels.\n","\n","#     Args:\n","#         eval_tuple (Tuple): Tuple containing logits and corresponding ground truth labels.\n","\n","#     Returns:\n","#         Dict: Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n","#     \"\"\"\n","#     logits, labels = eval_tuple\n","\n","#     # Calculate predictions\n","#     preds = logits.argmax(axis=-1)\n","# #     print(preds)\n","# #     print(\"Done\")\n","# #     print(\"\\n\")\n","# #     Compute metrics\n","#     metrics = {\n","#         \"eval_acc\": accuracy_score(labels, preds),\n","#         \"eval_f1\": f1_score(labels, preds, average='weighted')\n","#     }\n","#     return metrics\n","# # #     print( metric.compute(predictions=preds, references=labels))\n","# #     return metric.compute(predictions=preds, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Wrapper around the wup_measure(...) function to process batch inputs\n","# def batch_wup_measure(labels, preds):\n","#     wup_scores = [wup_measure(label_encoder(label), label_encoder(pred)) for label, pred in zip(labels, preds)]\n","#     return np.mean(wup_scores)\n","\n","# # Function to compute all relevant performance metrics, to be passed into the trainer\n","# def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n","#     logits, labels = eval_tuple\n","#     preds = logits.argmax(axis=-1)\n","#     return {\n","# #         \"wups\": batch_wup_measure(labels, preds),\n","#         \"acc\": accuracy_score(labels, preds),\n","#         \"f1\": f1_score(labels, preds, average='weighted')\n","#     }\n","from sklearn.metrics import recall_score\n","def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]):\n","    \"\"\"\n","    Computes evaluation metrics for a given set of logits and labels.\n","\n","    Args:\n","        eval_tuple (Tuple): Tuple containing logits and corresponding ground truth labels.\n","\n","    Returns:\n","        Dict: Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n","    \"\"\"\n","    logits, labels = eval_tuple\n","\n","    # Calculate predictions\n","    preds = logits.argmax(axis=-1)\n","#     print(preds)\n","#     print(\"Done\")\n","#     print(\"\\n\")\n","#     Compute metrics\n","    metrics = {\n","        \"eval_acc\": accuracy_score(labels, preds),\n","        \"eval_f1\": f1_score(labels, preds, average='weighted'),\n","        \"eval_precision\": precision_metric.compute(predictions=preds, references=labels,average=\"weighted\"),\n","#         \"eval_recall\": recall_metric.compute(prediction=preds, references=labels,average='macro'),\n","        \"eval_recall\": recall_score(labels, preds, average='weighted')\n","    }\n","    return metrics\n","# #     print( metric.compute(predictions=preds, references=labels))\n","#     return metric.compute(predictions=preds, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:50:04.991696Z","iopub.status.busy":"2024-05-17T12:50:04.991378Z","iopub.status.idle":"2024-05-17T12:50:05.009245Z","shell.execute_reply":"2024-05-17T12:50:05.007620Z","shell.execute_reply.started":"2024-05-17T12:50:04.991646Z"},"trusted":true},"outputs":[],"source":["# from transformers import AutoModel\n","# model2 = AutoModel.from_pretrained(\"/kaggle/input/vqamodel1/\", use_safetensors=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T12:50:05.011147Z","iopub.status.busy":"2024-05-17T12:50:05.010780Z","iopub.status.idle":"2024-05-17T17:44:51.270453Z","shell.execute_reply":"2024-05-17T17:44:51.268956Z","shell.execute_reply.started":"2024-05-17T12:50:05.011115Z"},"trusted":true},"outputs":[],"source":["\n","multi_args = TrainingArguments(\n","    output_dir=\"/kaggle/working/vilbert/\",\n","    seed=12345, \n","    evaluation_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_steps=100,\n","    save_total_limit=1,             # Since models are large, save only the last 3 checkpoints at any given time while training \n","    metric_for_best_model=\"acc\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    remove_unused_columns=False,\n","    num_train_epochs=5,\n","    fp16=False,\n","    dataloader_num_workers=4,\n","    load_best_model_at_end=True,\n","    report_to=\"tensorboard\",\n",")\n","\n","# Initialize the actual collator and multimodal model\n","collator, model = createMultimodalVQACollatorAndModel(\"bert-base-uncased\", \"google/vit-base-patch16-224-in21k\")\n","\n","# Initialize the trainer with the dataset, collator, model, hyperparameters and evaluation metrics\n","multi_trainer = Trainer(\n","\tmodel,\n","\tmulti_args,\n","\ttrain_dataset=dataset['train'],\n","\teval_dataset=dataset['test'],\n","\tdata_collator=collator,\n","\tcompute_metrics=compute_metrics\n","#     , callbacks = [TensorBoardCallback()]\n",")\n","\n","# Start the training loop\n","train_multi_metrics = multi_trainer.train()\n","model_path = os.path.join(\"/kaggle/working/vilbert/\", \"pytorch_model.bin\")\n","torch.save(model.state_dict(), model_path)\n","# Run the model on the evaluation set to obtain final metrics\n","eval_multi_metrics = multi_trainer.evaluate()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4950186,"sourceId":8423775,"sourceType":"datasetVersion"},{"datasetId":5016604,"sourceId":8425139,"sourceType":"datasetVersion"},{"datasetId":5027540,"sourceId":8439746,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
