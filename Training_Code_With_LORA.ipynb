{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T13:25:00.235624Z","iopub.status.busy":"2024-05-17T13:25:00.234924Z","iopub.status.idle":"2024-05-17T13:25:00.240565Z","shell.execute_reply":"2024-05-17T13:25:00.239518Z","shell.execute_reply.started":"2024-05-17T13:25:00.235589Z"},"trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:00.244969Z","iopub.status.busy":"2024-05-17T13:25:00.244065Z","iopub.status.idle":"2024-05-17T13:25:00.330867Z","shell.execute_reply":"2024-05-17T13:25:00.329916Z","shell.execute_reply.started":"2024-05-17T13:25:00.244941Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","import pickle\n","import nltk\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","from keras.models import Model\n","from keras.applications.vgg19 import VGG19, preprocess_input\n","from keras.utils import load_img, img_to_array\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from copy import deepcopy\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AutoTokenizer, AutoFeatureExtractor, AutoModel,            \n","    TrainingArguments, Trainer, logging\n",")\n","from datasets import load_dataset, set_caching_enabled, Dataset\n","# from nltk.corpus import wordnet\n","\n","# # nltk setup\n","# nltk.download('wordnet')\n","\n","# Environment setup\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","# os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","# set_caching_enabled(True)\n","# Logging setup\n","logging.set_verbosity_error()\n","\n","# Device setup\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:00.333732Z","iopub.status.busy":"2024-05-17T13:25:00.332871Z","iopub.status.idle":"2024-05-17T13:25:01.400945Z","shell.execute_reply":"2024-05-17T13:25:01.399857Z","shell.execute_reply.started":"2024-05-17T13:25:00.333696Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"/kaggle/input/sampled-dataset-for-vqa/sampled_train_dataset_kaggle.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:01.402880Z","iopub.status.busy":"2024-05-17T13:25:01.402314Z","iopub.status.idle":"2024-05-17T13:25:01.944023Z","shell.execute_reply":"2024-05-17T13:25:01.942966Z","shell.execute_reply.started":"2024-05-17T13:25:01.402849Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(\"/kaggle/input/sampled-dataset-for-vqa/sampled_validation_dataset_kaggle.csv\")\n","test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:01.947096Z","iopub.status.busy":"2024-05-17T13:25:01.946686Z","iopub.status.idle":"2024-05-17T13:25:01.962247Z","shell.execute_reply":"2024-05-17T13:25:01.961317Z","shell.execute_reply.started":"2024-05-17T13:25:01.947059Z"},"trusted":true},"outputs":[],"source":["list_vocabulary = list(df['multiple_choice_answer'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:01.963935Z","iopub.status.busy":"2024-05-17T13:25:01.963648Z","iopub.status.idle":"2024-05-17T13:25:01.968750Z","shell.execute_reply":"2024-05-17T13:25:01.967810Z","shell.execute_reply.started":"2024-05-17T13:25:01.963912Z"},"trusted":true},"outputs":[],"source":["def label_encoder(word):\n","    if word in list_vocabulary:\n","        index = list_vocabulary.index(word)\n","        return index\n","    else:\n","        return len(list_vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:01.970571Z","iopub.status.busy":"2024-05-17T13:25:01.970195Z","iopub.status.idle":"2024-05-17T13:25:04.307051Z","shell.execute_reply":"2024-05-17T13:25:04.306060Z","shell.execute_reply.started":"2024-05-17T13:25:01.970544Z"},"trusted":true},"outputs":[],"source":["df['labels'] = df['multiple_choice_answer'].apply(lambda x: label_encoder(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:04.308539Z","iopub.status.busy":"2024-05-17T13:25:04.308251Z","iopub.status.idle":"2024-05-17T13:25:05.423232Z","shell.execute_reply":"2024-05-17T13:25:05.422166Z","shell.execute_reply.started":"2024-05-17T13:25:04.308515Z"},"trusted":true},"outputs":[],"source":["test_df['labels'] = test_df['multiple_choice_answer'].apply(lambda x: label_encoder(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:05.424741Z","iopub.status.busy":"2024-05-17T13:25:05.424437Z","iopub.status.idle":"2024-05-17T13:25:11.652903Z","shell.execute_reply":"2024-05-17T13:25:11.651884Z","shell.execute_reply.started":"2024-05-17T13:25:05.424717Z"},"trusted":true},"outputs":[],"source":["df_train = df\n","df_test = test_df\n","df_train.to_csv(\"data_train.csv\", index=None)\n","df_test.to_csv(\"data_eval.csv\", index=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:11.654860Z","iopub.status.busy":"2024-05-17T13:25:11.654568Z","iopub.status.idle":"2024-05-17T13:25:13.720235Z","shell.execute_reply":"2024-05-17T13:25:13.719456Z","shell.execute_reply.started":"2024-05-17T13:25:11.654835Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\n","    \"csv\", \n","    data_files={\n","        \"train\": \"data_train.csv\",\n","        \"test\": \"data_eval.csv\"\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:13.723915Z","iopub.status.busy":"2024-05-17T13:25:13.723651Z","iopub.status.idle":"2024-05-17T13:25:26.054115Z","shell.execute_reply":"2024-05-17T13:25:26.052969Z","shell.execute_reply.started":"2024-05-17T13:25:13.723893Z"},"trusted":true},"outputs":[],"source":["!pip install peft -q"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.056246Z","iopub.status.busy":"2024-05-17T13:25:26.055857Z","iopub.status.idle":"2024-05-17T13:25:26.063263Z","shell.execute_reply":"2024-05-17T13:25:26.062329Z","shell.execute_reply.started":"2024-05-17T13:25:26.056208Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    init_lora_weights=\"gaussian\",\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"out_proj\"],\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    modules_to_save=[\"classifier\"],\n",")\n","config2 = LoraConfig(\n","    init_lora_weights=\"gaussian\",\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"query\", \"value\", \"linear\"],\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    modules_to_save=[\"classifier\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.064755Z","iopub.status.busy":"2024-05-17T13:25:26.064446Z","iopub.status.idle":"2024-05-17T13:25:26.237505Z","shell.execute_reply":"2024-05-17T13:25:26.236476Z","shell.execute_reply.started":"2024-05-17T13:25:26.064732Z"},"trusted":true},"outputs":[],"source":["# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_use_double_quant=True,\n","#     bnb_4bit_compute_dtype=torch.bfloat16,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.239191Z","iopub.status.busy":"2024-05-17T13:25:26.238878Z","iopub.status.idle":"2024-05-17T13:25:26.247236Z","shell.execute_reply":"2024-05-17T13:25:26.246379Z","shell.execute_reply.started":"2024-05-17T13:25:26.239166Z"},"trusted":true},"outputs":[],"source":["# from peft import replace_lora_weights_loftq\n","# from transformers import BitsAndBytesConfig\n","\n","\n","# base_model = MultimodalBert\n","# AttentionBERT.from_pretrained(..., quantization_config=bnb_config)\n","\n","# # note: don't pass init_lora_weights=\"loftq\" or loftq_config!\n","# lora_config = LoraConfig(task_type=\"CAUSAL_LM\")\n","# peft_model = get_peft_model(base_model, lora_config)\n","# replace_lora_weights_loftq(peft_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.249340Z","iopub.status.busy":"2024-05-17T13:25:26.248577Z","iopub.status.idle":"2024-05-17T13:25:26.256959Z","shell.execute_reply":"2024-05-17T13:25:26.256058Z","shell.execute_reply.started":"2024-05-17T13:25:26.249315Z"},"trusted":true},"outputs":[],"source":["# from peft import LoraConfig\n","# from transformers import BitsAndBytesConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.258508Z","iopub.status.busy":"2024-05-17T13:25:26.258195Z","iopub.status.idle":"2024-05-17T13:25:26.267447Z","shell.execute_reply":"2024-05-17T13:25:26.266640Z","shell.execute_reply.started":"2024-05-17T13:25:26.258476Z"},"trusted":true},"outputs":[],"source":["class FeedForwardNeuralNetwork(nn.Module):\n","    def __init__(self,embedding_dim):\n","        super(FeedForwardNeuralNetwork, self).__init__()\n","        self.fc1 = nn.Linear(embedding_dim,2*embedding_dim)\n","        self.fc2 = nn.Linear(2*embedding_dim,embedding_dim)\n","        self.activation_function = nn.GELU()\n","#         self.layer_norm = nn.LayerNorm(embedding_dim)\n","    def forward(self,input_data):\n","        output = self.fc1(input_data)\n","        output = self.activation_function(output)\n","        output = self.fc2(output)\n","        output = self.activation_function(output)\n","#         output = self.layer_norm(output)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.269233Z","iopub.status.busy":"2024-05-17T13:25:26.268900Z","iopub.status.idle":"2024-05-17T13:25:26.282719Z","shell.execute_reply":"2024-05-17T13:25:26.282012Z","shell.execute_reply.started":"2024-05-17T13:25:26.269204Z"},"trusted":true},"outputs":[],"source":["class AttentionBert(nn.Module):\n","    def __init__(self,hidden_size, num_heads):\n","        super(AttentionBert, self).__init__()\n","#         self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.layer_norm1 = nn.LayerNorm(self.hidden_size).to(device)\n","        self.ffn = FeedForwardNeuralNetwork(self.hidden_size).to(device)\n","#         self.query_linear = nn.Linear(input_size, hidden_size)\n","#         self.key_linear = nn.Linear(input_size, hidden_size)\n","#         self.value_linear = nn.Linear(input_size, hidden_size)\n","        \n","        self.multihead_attention = nn.MultiheadAttention(self.hidden_size, self.num_heads).to(device)\n","        print(self.multihead_attention)\n","    def forward(self, query,input_features):\n","        # Transform query, key, and value\n","#         query = self.query_linear(query)\n","#         key = self.key_linear(input_features)\n","#         value = self.value_linear(input_features)\n","        query = query\n","        key = input_features\n","        value = input_features\n","        \n","        # Transpose for multihead attention\n","        query = query.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        key = key.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        value = value.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","#         print(query.get_device())\n","#         print(key.get_device())\n","#         print(value.get_device())\n","        # Compute co-attention and add to the query of the layer\n","        co_attention_output, _ = self.multihead_attention(query, key, value)\n","#         query = query.transpose(0, 1)\n","        residual_output = torch.add(co_attention_output,query)\n","        \n","        # Transpose back to original shape and normalize\n","        residual_output = residual_output.transpose(0, 1) # (batch_size, seq_len, hidden_size)\n","#         print(residual_output.get_device())\n","        normalized_residual_output = self.layer_norm1(residual_output)\n","        \n","        #Send input to feedforward neural network and add and nor\n","        feedforwardoutput = self.ffn.forward(normalized_residual_output)\n","        residual_output2 = torch.add(feedforwardoutput,normalized_residual_output)\n","#         print(residual_output2.get_device())\n","        normalized_residual_output2 = self.layer_norm1(residual_output2)\n","        \n","        return normalized_residual_output2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.284185Z","iopub.status.busy":"2024-05-17T13:25:26.283912Z","iopub.status.idle":"2024-05-17T13:25:26.296211Z","shell.execute_reply":"2024-05-17T13:25:26.295468Z","shell.execute_reply.started":"2024-05-17T13:25:26.284162Z"},"trusted":true},"outputs":[],"source":["class LoRALinear(nn.Module):\n","    def __init__(self, in_features, out_features, rank=4):\n","        super(LoRALinear, self).__init__()\n","        self.rank = rank\n","        self.lora_A = nn.Parameter(torch.randn(in_features, rank))\n","        self.lora_B = nn.Parameter(torch.randn(rank, out_features))\n","        self.lora_dropout = nn.Dropout(p=0.1)\n","        self.scaling = 1 / (rank ** 0.5)\n","    \n","    def forward(self, x):\n","        lora_output = self.lora_dropout(x @ self.lora_A) @ self.lora_B\n","        return self.scaling * lora_output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.297575Z","iopub.status.busy":"2024-05-17T13:25:26.297246Z","iopub.status.idle":"2024-05-17T13:25:26.309545Z","shell.execute_reply":"2024-05-17T13:25:26.308723Z","shell.execute_reply.started":"2024-05-17T13:25:26.297552Z"},"trusted":true},"outputs":[],"source":["# get_peft_model(base_model, lora_config)\n","class LoRAAttentionBert(AttentionBert):\n","#     def __init__(self, hidden_size, num_heads, lora_config=config):\n","#         super(LoRAAttentionBert, self).__init__(hidden_size, num_heads)\n","#         self.lora_config = lora_config\n","#         # Add LoRA configuration here\n","\n","#     def forward(self, query, input_features):\n","#         # Call the forward method of the parent class\n","#         output = super().forward(query, input_features)\n","#         # Add LoRA operations here if needed\n","#         return output\n","    \n","    def __init__(self, hidden_size, num_heads, lora_rank=4):\n","        super(AttentionBert, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        \n","        self.layer_norm1 = nn.LayerNorm(self.hidden_size).to(device)\n","        self.ffn = FeedForwardNeuralNetwork(self.hidden_size).to(device)\n","        self.multihead_attention = nn.MultiheadAttention(self.hidden_size, self.num_heads).to(device)\n","        \n","        # LoRA layers\n","        self.lora_query = LoRALinear(hidden_size, hidden_size, lora_rank).to(device)\n","        self.lora_key = LoRALinear(hidden_size, hidden_size, lora_rank).to(device)\n","        self.lora_value = LoRALinear(hidden_size, hidden_size, lora_rank).to(device)\n","    \n","    def forward(self, query, input_features):\n","        # Apply LoRA to query, key, and value\n","        query_lora = self.lora_query(query)\n","        key_lora = self.lora_key(input_features)\n","        value_lora = self.lora_value(input_features)\n","\n","        # Combine original features with LoRA adaptations\n","        query = query + query_lora\n","        key = input_features + key_lora\n","        value = input_features + value_lora\n","\n","        # Transpose for multihead attention\n","        query = query.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        key = key.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        value = value.transpose(0, 1)  # (seq_len, batch_size, hidden_size)\n","        \n","        # Compute co-attention and add to the query of the layer\n","        co_attention_output, _ = self.multihead_attention(query, key, value)\n","        residual_output = torch.add(co_attention_output, query)\n","        \n","        # Transpose back to original shape and normalize\n","        residual_output = residual_output.transpose(0, 1) # (batch_size, seq_len, hidden_size)\n","        normalized_residual_output = self.layer_norm1(residual_output)\n","        \n","        # Send input to feedforward neural network and add and normalize\n","        feedforward_output = self.ffn(normalized_residual_output)\n","        residual_output2 = torch.add(feedforward_output, normalized_residual_output)\n","        normalized_residual_output2 = self.layer_norm1(residual_output2)\n","        \n","        return normalized_residual_output2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.311182Z","iopub.status.busy":"2024-05-17T13:25:26.310839Z","iopub.status.idle":"2024-05-17T13:25:26.323435Z","shell.execute_reply":"2024-05-17T13:25:26.322614Z","shell.execute_reply.started":"2024-05-17T13:25:26.311131Z"},"trusted":true},"outputs":[],"source":["class SelfTRM(nn.Module):\n","    def __init__(self,num_layers,num_heads,input_size):\n","        super(SelfTRM,self).__init__()\n","        self.num_layers = num_layers\n","        self.layers = []\n","        for i in range(self.num_layers):\n","#             self.layers.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n","#             AttentionBert(input_size,num_heads)\n","# get_peft_model(AttentionBert(input_size,num_heads),config)\n","            self.layers.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n","#         print(self.layers[0])\n","#             self.layers.append(LoRAAttentionBert(input_size,num_heads))\n","    def forward(self,input):\n","#         print(input.get_device())\n","        for i in range(self.num_layers):\n","            output = self.layers[i].forward(input,input)\n","            input = output\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.325068Z","iopub.status.busy":"2024-05-17T13:25:26.324743Z","iopub.status.idle":"2024-05-17T13:25:26.334718Z","shell.execute_reply":"2024-05-17T13:25:26.333943Z","shell.execute_reply.started":"2024-05-17T13:25:26.325039Z"},"trusted":true},"outputs":[],"source":["class MLP_classifier(nn.Module):\n","    def __init__(self,input_size,output_size):\n","        super(MLP_classifier,self).__init__()\n","        self.linear1 = nn.Linear(input_size,input_size*2)\n","        self.activation_function1 =  nn.GELU()\n","        self.linear2 = nn.Linear(input_size*2,output_size)\n","        self.activation_function2 = nn.Softmax()\n","    def forward(self,input):\n","        output = self.linear1(input)\n","        output = self.activation_function1(output)\n","        output = self.linear2(output)\n","        return output\n","#         output = self.activation_function2(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.336261Z","iopub.status.busy":"2024-05-17T13:25:26.336005Z","iopub.status.idle":"2024-05-17T13:25:26.350014Z","shell.execute_reply":"2024-05-17T13:25:26.349161Z","shell.execute_reply.started":"2024-05-17T13:25:26.336240Z"},"trusted":true},"outputs":[],"source":["class BiattentionforImageandWord(nn.Module):\n","    def __init__(self,num_layers,num_heads,input_size):\n","        super(BiattentionforImageandWord,self).__init__()\n","        self.layers_image = []\n","        self.layers_word = []\n","        self.input_size = input_size\n","        self.num_heads = num_heads\n","        self.num_layers = num_layers\n","#         get_peft_model(AttentionBert(input_size,num_heads),config)\n","        for i in range(self.num_layers):\n","#             \n","#             self.layers_image.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n","            self.layers_image.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n","#             self.layers_image.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n","            self.layers_image.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n","#             self.layers_word.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n","            self.layers_word.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n","#             self.layers_word.append(replace_lora_weights_loftq(get_peft_model(AttentionBert(input_size,num_heads),config)))\n","            self.layers_word.append(get_peft_model(AttentionBert(input_size,num_heads),config))\n","    def forward(self,query_image,key_image,query_word,key_word):\n","        for i in range(self.num_layers):\n","            output_image1 = self.layers_image[2*i].forward(query_image,key_word)\n","            output_image2 = self.layers_image[2*i+1].forward(output_image1,output_image1)\n","            output_word1 = self.layers_word[2*i].forward(query_word,key_image)\n","            output_word2 = self.layers_word[2*i+1].forward(output_word1,output_word1)\n","            query_image = key_image = output_image2\n","            query_word = key_word = output_word2\n","        return output_image2,output_word2    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.351567Z","iopub.status.busy":"2024-05-17T13:25:26.351231Z","iopub.status.idle":"2024-05-17T13:25:26.365310Z","shell.execute_reply":"2024-05-17T13:25:26.364515Z","shell.execute_reply.started":"2024-05-17T13:25:26.351544Z"},"trusted":true},"outputs":[],"source":["class MultimodalBert(nn.Module):\n","    def __init__(self,text,image,num_layers,num_heads,input_size,output_size):\n","        super(MultimodalBert,self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.word_model = AutoModel.from_pretrained(text)\n","        self.image_model = AutoModel.from_pretrained(image)\n","        self.CoTRM = BiattentionforImageandWord(num_layers,num_heads,self.input_size)\n","        self.wordTRM = SelfTRM(num_layers,num_heads,self.input_size)\n","        self.MLPclassifier = MLP_classifier(self.input_size*2,self.output_size)\n","        self.criterion = nn.CrossEntropyLoss()\n","    def forward(self,\n","            input_ids: torch.LongTensor,\n","            pixel_values: torch.FloatTensor,\n","            attention_mask: Optional[torch.LongTensor] = None,\n","            token_type_ids: Optional[torch.LongTensor] = None,\n","            labels: Optional[torch.LongTensor] = None):\n","        output = self.word_model(input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            return_dict=True,)\n","#                 output =  self.word_model(**encoded_text)\n","        word_features = output.last_hidden_state\n","        output = self.image_model(\n","            pixel_values=pixel_values,\n","            return_dict=True,\n","        )\n","        image_features = output.last_hidden_state\n","        #     (self,image_features,word_features,labels = None):\n","        \n","        #Self Transformer before CoTRM\n","        self_trm_output = self.wordTRM.forward(word_features)\n","        #CoTRM Portion\n","        CoTRM_word_input = self_trm_output \n","        CoTRM_image_input = image_features\n","        CoTRM_output_image,CoTRM_output_word = self.CoTRM.forward(CoTRM_image_input,CoTRM_image_input,CoTRM_word_input,CoTRM_word_input)\n","        # Merged output and MLP classifier\n","        merged_input = torch.cat([CoTRM_output_image[:,0,:],CoTRM_output_word[:,0,:]],dim=1)\n","        final_output = self.MLPclassifier.forward(merged_input)\n","#         print(final_output.predictions)\n","#         class_outputs = np.argmax(final_output,axis = 1)\n","        out = {\n","            \"out\": final_output\n","        }\n","        if labels is not None:\n","            loss = self.criterion(final_output, labels)\n","            out[\"loss\"] = loss\n","#             out[\"labels\"] = labels\n","#         print(\"Printing Labels\")\n","#         print(labels)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.366874Z","iopub.status.busy":"2024-05-17T13:25:26.366618Z","iopub.status.idle":"2024-05-17T13:25:26.380661Z","shell.execute_reply":"2024-05-17T13:25:26.379739Z","shell.execute_reply.started":"2024-05-17T13:25:26.366853Z"},"trusted":true},"outputs":[],"source":["# The dataclass decorator is used to automatically generate special methods to classes, \n","# including __init__, __str__ and __repr__. It helps reduce some boilerplate code.\n","@dataclass\n","class MultimodalCollator:\n","    tokenizer: AutoTokenizer\n","\n","    preprocessor: AutoFeatureExtractor\n","        \n","    def tokenize_text(self, texts: List[str]):\n","        #Encoded text \n","        encoded_text = self.tokenizer(\n","            text=texts, \n","            padding='longest',\n","            max_length=40,\n","            truncation=True,\n","            return_tensors='pt',\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","        )\n","#         output =  self.word_model(**encoded_text)\n","#         word_features = output.last_hidden_state\n","#         return {\n","#             \"word_features\": word_features\n","#         }\n","        return {\n","                \"input_ids\": encoded_text['input_ids'].squeeze(),\n","#                 \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n","#                 \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n","            }\n","\n","    def preprocess_images(self, images: List[str]):\n","        #Fixed\n","#         print(\"Done\")\n","        \n","        processed_images = self.preprocessor(\n","            images=[Image.open(os.path.join(\"/kaggle/input/sampled-dataset-for-vqa\", image_name.replace(\"\\\\\", \"/\"))).convert('RGB') for image_name in images],\n","            return_tensors=\"pt\",\n","        )\n","#         print(\"done2\")\n","#         output = self.image_model(**processed_images) #Of the form (Num_images,Num_tokens,Hidden_size)\n","#         image_features = output.last_hidden_state\n","#         return{\n","#             \"image_features\": image_features\n","#         }\n","        return {\n","            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n","        }\n","            \n","    def __call__(self, raw_batch_dict):\n","        #Fix the label return value\n","        return {\n","            **self.tokenize_text(\n","                raw_batch_dict['question']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['question'] for i in raw_batch_dict]\n","            ),\n","            **self.preprocess_images(\n","                raw_batch_dict[\"kaggle_image_path\"]\n","                if isinstance(raw_batch_dict, dict) else\n","                [i[\"kaggle_image_path\"] for i in raw_batch_dict]\n","            ),\n","            \"labels\": torch.tensor(\n","                raw_batch_dict['labels']\n","                if isinstance(raw_batch_dict, dict) else\n","                [i['labels'] for i in raw_batch_dict],\n","                dtype=torch.int64\n","            ),\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.382451Z","iopub.status.busy":"2024-05-17T13:25:26.381857Z","iopub.status.idle":"2024-05-17T13:25:26.408249Z","shell.execute_reply":"2024-05-17T13:25:26.407365Z","shell.execute_reply.started":"2024-05-17T13:25:26.382418Z"},"trusted":true},"outputs":[],"source":["def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k',num_layers = 3,num_heads=3,hidden_size = 768,vocabulary_size =9129):\n","    # Initialize the correct text tokenizer and image feature extractor, and use them to create the collator\n","    tokenizer = AutoTokenizer.from_pretrained(text)\n","#     word_model = AutoModel.from_pretrained(text)\n","    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n","#     image_model = AutoModel.from_pretrained(image)\n","    multimodal_collator = MultimodalCollator(tokenizer=tokenizer, preprocessor=preprocessor)\n","    \n","    # Initialize the multimodal model with the appropriate weights from pretrained models\n","    multimodal_model = MultimodalBert(text,image,num_layers = num_layers,num_heads=num_heads,input_size = hidden_size,output_size = vocabulary_size).to(device)\n","#     multimodal_model = multimodal_model.to(device)\n","    return multimodal_collator, multimodal_model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.409603Z","iopub.status.busy":"2024-05-17T13:25:26.409324Z","iopub.status.idle":"2024-05-17T13:25:26.418587Z","shell.execute_reply":"2024-05-17T13:25:26.417755Z","shell.execute_reply.started":"2024-05-17T13:25:26.409581Z"},"trusted":true},"outputs":[],"source":["# def explore_module(module):\n","#     for name, sub_module in module.named_childre():\n","#         print(name)\n","#         print(sub_module)\n","#         print()\n","#         explore_module(sub_module)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.419849Z","iopub.status.busy":"2024-05-17T13:25:26.419596Z","iopub.status.idle":"2024-05-17T13:25:26.427417Z","shell.execute_reply":"2024-05-17T13:25:26.426758Z","shell.execute_reply.started":"2024-05-17T13:25:26.419827Z"},"trusted":true},"outputs":[],"source":["# multimodal_model = MultimodalBert(text='bert-base-uncased',image='google/vit-base-patch16-224-in21k',num_layers = 3,num_heads=3,input_size = 768,output_size =9129)\n","# explore_module(multimodal_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:26.428836Z","iopub.status.busy":"2024-05-17T13:25:26.428583Z","iopub.status.idle":"2024-05-17T13:25:39.198714Z","shell.execute_reply":"2024-05-17T13:25:39.197696Z","shell.execute_reply.started":"2024-05-17T13:25:26.428814Z"},"trusted":true},"outputs":[],"source":["!pip install evaluate\n","\n","import evaluate\n","\n","metric = evaluate.load(\"accuracy\")\n","precision_metric = evaluate.load(\"precision\")\n","recall_metric = evaluate.load('recall')\n","accuracy_metric = evaluate.load(\"accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:39.204181Z","iopub.status.busy":"2024-05-17T13:25:39.203869Z","iopub.status.idle":"2024-05-17T13:25:39.211702Z","shell.execute_reply":"2024-05-17T13:25:39.210804Z","shell.execute_reply.started":"2024-05-17T13:25:39.204155Z"},"trusted":true},"outputs":[],"source":["# # Wrapper around the wup_measure(...) function to process batch inputs\n","# # def batch_wup_measure(labels, preds):\n","# #     wup_scores = [wup_measure(label_encoder(label), label_encoder(pred)) for label, pred in zip(labels, preds)]\n","# #     return np.mean(wup_scores)\n","\n","# # # Function to compute all relevant performance metrics, to be passed into the trainer\n","# # def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n","# #     logits, labels = eval_tuple\n","# #     preds = logits.argmax(axis=-1)\n","# #     return {\n","# # #         \"wups\": batch_wup_measure(labels, preds),\n","# #         \"acc\": accuracy_score(labels, preds),\n","# #         \"f1\": f1_score(labels, preds, average='weighted')\n","# #     }\n","# def compute_metric(eval_tuple: Tuple[np.ndarray, np.ndarray]):\n","#     \"\"\"\n","#     Computes evaluation metrics for a given set of logits and labels.\n","\n","#     Args:\n","#         eval_tuple (Tuple): Tuple containing logits and corresponding ground truth labels.\n","\n","#     Returns:\n","#         Dict: Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n","#     \"\"\"\n","#     logits, labels = eval_tuple\n","# #     print(\"Done Won\")\n","\n","#     # Calculate predictions\n","#     #     print(preds)\n","#     #     print(\"Done\")\n","#     #     print(\"\\n\")\n","#     #     Compute metrics\n","#     #     metrics = {\n","#     #         \"eval_acc\": accuracy_score(labels, preds),\n","#     #         \"eval_f1\": f1_score(labels, preds, average='weighted')\n","#     #     }\n","#     #     return metrics\n","# #     print(\"Printing logits\")\n","# #     print(logits)\n","#     preds = logits.argmax(axis=-1)\n","# #     print(metric.compute(predictions=preds, references=labels))\n","#     return metric.compute(predictions=preds, references=labels)\n","\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Wrapper around the wup_measure(...) function to process batch inputs\n","# def batch_wup_measure(labels, preds):\n","#     wup_scores = [wup_measure(label_encoder(label), label_encoder(pred)) for label, pred in zip(labels, preds)]\n","#     return np.mean(wup_scores)\n","\n","# # Function to compute all relevant performance metrics, to be passed into the trainer\n","# def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n","#     logits, labels = eval_tuple\n","#     preds = logits.argmax(axis=-1)\n","#     return {\n","# #         \"wups\": batch_wup_measure(labels, preds),\n","#         \"acc\": accuracy_score(labels, preds),\n","#         \"f1\": f1_score(labels, preds, average='weighted')\n","#     }\n","from sklearn.metrics import recall_score\n","def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]):\n","    \"\"\"\n","    Computes evaluation metrics for a given set of logits and labels.\n","\n","    Args:\n","        eval_tuple (Tuple): Tuple containing logits and corresponding ground truth labels.\n","\n","    Returns:\n","        Dict: Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n","    \"\"\"\n","    logits, labels = eval_tuple\n","\n","    # Calculate predictions\n","    preds = logits.argmax(axis=-1)\n","#     print(preds)\n","#     print(\"Done\")\n","#     print(\"\\n\")\n","#     Compute metrics\n","    metrics = {\n","        \"eval_acc\": accuracy_metric.compute(predictions=preds, references=labels),\n","        \"eval_f1\": f1_score(labels, preds, average='weighted'),\n","        \"eval_precision\": precision_metric.compute(predictions=preds, references=labels,average='weighted'),\n","        \"eval_recall\": recall_score(labels, preds, average='weighted'),\n","    }\n","    return metrics\n","# #     print( metric.compute(predictions=preds, references=labels))\n","#     return metric.compute(predictions=preds, references=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T13:25:39.213250Z","iopub.status.busy":"2024-05-17T13:25:39.212964Z","iopub.status.idle":"2024-05-17T17:25:59.931735Z","shell.execute_reply":"2024-05-17T17:25:59.930508Z","shell.execute_reply.started":"2024-05-17T13:25:39.213227Z"},"trusted":true},"outputs":[],"source":["multi_args = TrainingArguments(\n","    output_dir=\"checkpoint\",\n","    seed=12345, \n","    evaluation_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_steps=1000,\n","    save_total_limit=1,             # Since models are large, save only the last 3 checkpoints at any given time while training \n","    metric_for_best_model=\"accuracy\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    remove_unused_columns=False,\n","    num_train_epochs=5,\n","    fp16=False,\n","    dataloader_num_workers=4,\n","    load_best_model_at_end=True,\n","    report_to=\"tensorboard\",\n","    label_names=[\"labels\"]\n",")\n","\n","# Initialize the actual collator and multimodal model\n","collator, model = createMultimodalVQACollatorAndModel(\"bert-base-uncased\", \"google/vit-base-patch16-224-in21k\")\n","# model = replace_lora_weights_loftq(get_peft_model(model,config2))\n","lora_model = get_peft_model(model,config2)\n","# lora_model = model\n","print(lora_model)\n","\n","def print_trainable_parameters(model):\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","    )\n","    \n","print_trainable_parameters(model)\n","\n","# Initialize the trainer with the dataset, collator, model, hyperparameters and evaluation metrics\n","multi_trainer = Trainer(\n","\tlora_model,\n","\tmulti_args,\n","\ttrain_dataset=dataset['train'],\n","\teval_dataset=dataset['test'],\n","\tdata_collator=collator,\n","\tcompute_metrics=compute_metrics,\n",")\n","print(multi_trainer.compute_metrics)\n","# for batch in multi_trainer.get_eval_dataloader():\n","#     print(batch)\n","#     break\n","# Start the training loop\n","train_multi_metrics = multi_trainer.train()\n","\n","# Run the model on the evaluation set to obtain final metrics\n","eval_multi_metrics = multi_trainer.evaluate()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T17:30:03.484585Z","iopub.status.busy":"2024-05-17T17:30:03.484196Z","iopub.status.idle":"2024-05-17T17:30:05.602191Z","shell.execute_reply":"2024-05-17T17:30:05.600407Z","shell.execute_reply.started":"2024-05-17T17:30:03.484554Z"},"trusted":true},"outputs":[],"source":["model_path = os.path.join(\"/kaggle/working/\", \"pytorch_model_lora8.bin\")\n","torch.save(lora_model.state_dict(), model_path)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4950186,"sourceId":8423775,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
